{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No es necesaria. •\\tRecopilación, procesamiento y análisis de información cualitativa y cuantitativa: La recopilación de información se aborda a través de la interacción con clientes, la revisión de documentación interna y la búsqueda de información pública. Posteriormente la misma es procesada en conjunto con los demás consultores con el objetivo de generar conclusiones que se traduzcan en acciones específicas.\\n•\\tAsistencia en el desarrollo de publicaciones: Periódicamente se realizan estudios relacionados a mercados financieros y de capitales que son publicados en diarios y revistas de reconocido prestigio. \\n•\\tAsistencia en el desarrollo de proyectos: En su mayoría, los mismos están dirigidos a identificar ámbitos de mejora en materia de mejores prácticas de Gobierno Corporativo, y eventualmente guiar su implementación. Ello aplica tanto para entidades públicas y privadas. Analista Junior de Finanzas y Gobierno Corporativo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data from CSV\n",
    "import csv\n",
    "data = []\n",
    "with open('data.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)    \n",
    "    for row in reader:\n",
    "        text = \" \".join(row.values())\n",
    "        data.append(text)\n",
    "        \n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no es necesaria.  \\trecopilación, procesamiento y análisis de información cualitativa y cuantitativa: la recopilación de información se aborda a través de la interacción con clientes, la revisión de documentación interna y la búsqueda de información pública. posteriormente la misma es procesada en conjunto con los demás consultores con el objetivo de generar conclusiones que se traduzcan en acciones específicas.\\n \\tasistencia en el desarrollo de publicaciones: periódicamente se realizan estudios relacionados a mercados financieros y de capitales que son publicados en diarios y revistas de reconocido prestigio. \\n \\tasistencia en el desarrollo de proyectos: en su mayoría, los mismos están dirigidos a identificar ámbitos de mejora en materia de mejores prácticas de gobierno corporativo, y eventualmente guiar su implementación. ello aplica tanto para entidades públicas y privadas. analista junior de finanzas y gobierno corporativo'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "punctuations = ['•','/']\n",
    "translator = str.maketrans(\"\".join(punctuations),' '*len(punctuations))\n",
    "\n",
    "proc_data = []\n",
    "for text in data:\n",
    "    text = text.lower()\n",
    "    text = text.translate(translator)\n",
    "    proc_data.append(text)\n",
    "    \n",
    "data = proc_data\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/howl24/.virtualenvs/TextClassification/lib/python3.5/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicia revision\n",
      " Avance:  0  Tam. terms.:  0\n",
      " Avance:  100  Tam. terms.:  1778\n",
      " Excede tiempo en:  119\n",
      " Excede tiempo en:  132\n",
      " Avance:  200  Tam. terms.:  2486\n",
      " Avance:  300  Tam. terms.:  3081\n",
      " Avance:  400  Tam. terms.:  3570\n",
      " Avance:  500  Tam. terms.:  4059\n",
      " Excede tiempo en:  525\n",
      " Avance:  600  Tam. terms.:  4462\n",
      " Excede tiempo en:  645\n",
      " Avance:  700  Tam. terms.:  4799\n",
      " Excede tiempo en:  730\n",
      " Avance:  800  Tam. terms.:  5157\n",
      " Excede tiempo en:  839\n",
      " Avance:  900  Tam. terms.:  5504\n",
      " Avance:  1000  Tam. terms.:  5745\n",
      " Excede tiempo en:  1001\n",
      " Excede tiempo en:  1087\n",
      " Excede tiempo en:  1097\n",
      " Avance:  1100  Tam. terms.:  6095\n",
      " Excede tiempo en:  1163\n",
      " Avance:  1200  Tam. terms.:  6401\n",
      " Avance:  1300  Tam. terms.:  6708\n",
      " Excede tiempo en:  1311\n",
      " Avance:  1400  Tam. terms.:  6929\n",
      " Excede tiempo en:  1418\n",
      " Excede tiempo en:  1459\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the java file!\nUse software specific configuration paramaters or set the JAVAHOME environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-add427b9aade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mtag_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspanish_postagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_terms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TextClassification/lib/python3.5/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TextClassification/lib/python3.5/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mdefault_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_java_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mconfig_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Create a temporary input file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TextClassification/lib/python3.5/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mconfig_java\u001b[0;34m(bin, options, verbose)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_java_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_java_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0m_java_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'JAVAHOME'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'JAVA_HOME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'java.exe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TextClassification/lib/python3.5/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[1;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 604\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32m~/.virtualenvs/TextClassification/lib/python3.5/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 598\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TextClassification/lib/python3.5/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         (filename, url))\n\u001b[1;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the java file!\nUse software specific configuration paramaters or set the JAVAHOME environment variable.\n==========================================================================="
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "import multiprocessing\n",
    "import signal\n",
    "\n",
    "class TimeoutError (RuntimeError):\n",
    "    pass\n",
    "\n",
    "def handler (signum, frame):\n",
    "    raise TimeoutError()\n",
    "\n",
    "signal.signal (signal.SIGALRM, handler)\n",
    "\n",
    "def divide_list(n_chunks, list_to_divide):\n",
    "    chunk_size = len(list_to_divide)//n_chunks + 1\n",
    "    chunks = [list_to_divide[i:i+chunk_size] for i in range(0, len(list_to_divide), chunk_size)]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "spanish_postagger = StanfordPOSTagger('models/spanish.tagger', './stanford-postagger.jar')\n",
    "\n",
    "terms_by_tag = {}\n",
    "terms_by_tag['n'] = set()\n",
    "terms_by_tag['v'] = set()\n",
    "terms_by_tag['a'] = set()\n",
    "terms_by_tag['c'] = set()\n",
    "terms_by_tag['d'] = set()\n",
    "terms_by_tag['f'] = set()\n",
    "terms_by_tag['i'] = set()\n",
    "terms_by_tag['p'] = set()\n",
    "terms_by_tag['r'] = set()\n",
    "terms_by_tag['s'] = set()\n",
    "terms_by_tag['w'] = set()\n",
    "terms_by_tag['z'] = set()\n",
    "\n",
    "tags_by_term = {}\n",
    "\n",
    "print(\"Inicia revision\")\n",
    "for idx, text in enumerate(data):\n",
    "    if idx % 100 == 0:\n",
    "        print(\" Avance: \", idx, \" Tam. terms.: \", len(terms_by_tag['n']) + len(terms_by_tag['v']))\n",
    "    try:\n",
    "        signal.alarm(4)\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        new_words = False\n",
    "        for word in words:\n",
    "            if word not in tags_by_term:\n",
    "                new_words = True\n",
    "\n",
    "        if new_words: \n",
    "            tag_terms = spanish_postagger.tag(words)\n",
    "            for term in tag_terms:\n",
    "                word = term[0]\n",
    "                tag = term[1][0]\n",
    "                \n",
    "                terms_by_tag[tag].add(word)\n",
    "                tags_by_term[word] = tag\n",
    "\n",
    "    except TimeoutError as ex:\n",
    "        print(\" Excede tiempo en: \", idx)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    }
   ],
   "source": [
    "for tag in terms_by_tag:\n",
    "    if \"procesamiento\" in terms_by_tag[tag]:\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "#vocab.update(dict.fromkeys(terms_by_tag['n'], 0))\n",
    "#vocab.update(dict.fromkeys(terms_by_tag['v'], 0))\n",
    "#vocab.update(dict.fromkeys(terms_by_tag['a'], 0))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44011"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize, ngram_range=(1, 4), min_df=15, max_df=0.5)\n",
    "vectorizer.fit(data)\n",
    "vect_vocab = vectorizer.get_feature_names()\n",
    "vect_vocab = set(vect_vocab)\n",
    "len(vect_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4172"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n #v #a\n",
    "for phrase in vect_vocab:\n",
    "    words = phrase.split()\n",
    "    if len(words) == 1:\n",
    "        try:\n",
    "            if (tags_by_term[words[0]] == 'n' or\n",
    "                tags_by_term[words[0]] == 'v' or\n",
    "                tags_by_term[words[0]] == 'a'):\n",
    "                #print(phrase)\n",
    "                vocab[phrase] = 0                \n",
    "        except:\n",
    "            pass      \n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5760"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n1 + s + n2\n",
    "for phrase in vect_vocab:\n",
    "    words = phrase.split()\n",
    "    if len(words) == 3:\n",
    "        try:\n",
    "            if (tags_by_term[words[0]] == 'n' and\n",
    "                tags_by_term[words[1]] == 's' and\n",
    "                tags_by_term[words[2]] == 'n'):\n",
    "                #print(phrase)\n",
    "                vocab[phrase] = 0                \n",
    "        except:\n",
    "            pass      \n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6037"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n + c + n\n",
    "\n",
    "for phrase in vect_vocab:\n",
    "    words = phrase.split()\n",
    "    if len(words) == 3:\n",
    "        try:\n",
    "            if (tags_by_term[words[0]] == 'n' and\n",
    "                tags_by_term[words[1]] == 'c' and\n",
    "                tags_by_term[words[2]] == 'n'):\n",
    "                vocab[phrase] = 0\n",
    "                #print(phrase)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6057"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n + c + n + a\n",
    "\n",
    "for phrase in vect_vocab:\n",
    "    words = phrase.split()\n",
    "    if len(words) == 4:\n",
    "        try:\n",
    "            if (tags_by_term[words[0]] == 'n' and\n",
    "                tags_by_term[words[1]] == 'c' and\n",
    "                tags_by_term[words[2]] == 'n' and\n",
    "                tags_by_term[words[3]] == 'a'):\n",
    "                vocab[phrase] = 0\n",
    "                #print(phrase)\n",
    "        except:\n",
    "            pass\n",
    "len(vocab)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6126"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v + s + n\n",
    "for phrase in vect_vocab:\n",
    "    words = phrase.split()\n",
    "    if len(words) == 3:\n",
    "        try:\n",
    "            if (tags_by_term[words[0]] == 'v' and\n",
    "                tags_by_term[words[1]] == 's' and                \n",
    "                tags_by_term[words[2]] == 'n'):\n",
    "                vocab[phrase] = 0\n",
    "                #print(phrase)\n",
    "        except:\n",
    "            pass\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6380"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v + n\n",
    "for phrase in vect_vocab:\n",
    "    words = phrase.split()\n",
    "    if len(words) == 2:\n",
    "        try:\n",
    "            if (tags_by_term[words[0]] == 'v' and                \n",
    "                tags_by_term[words[1]] == 'n'):\n",
    "                vocab[phrase] = 0\n",
    "                #print(phrase)\n",
    "        except:\n",
    "            pass\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7401"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n + a\n",
    "for phrase in vect_vocab:\n",
    "    words = phrase.split()\n",
    "    if len(words) == 2:\n",
    "        try:\n",
    "            if (tags_by_term[words[0]] == 'n' and                \n",
    "                tags_by_term[words[1]] == 'a'):\n",
    "                vocab[phrase] = 0\n",
    "                #print(phrase)\n",
    "        except:\n",
    "            pass\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7401"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n + a + c + a\n",
    "for phrase in vect_vocab:\n",
    "    words = phrase.split()\n",
    "    if len(words) == 2:\n",
    "        try:\n",
    "            if (tags_by_term[words[0]] == 'n' and\n",
    "                tags_by_term[words[1]] == 'a' and\n",
    "                tags_by_term[words[2]] == 'c' and\n",
    "                tags_by_term[words[3]] == 'a'):\n",
    "                vocab[phrase] = 0\n",
    "                print(phrase)\n",
    "        except:\n",
    "            pass\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('frases_15rep.csv', 'w') as csvfile:\n",
    "    fn = ['Frase', 'Aceptar?']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fn)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    for row in vocab:\n",
    "        writer.writerow({'Frase': row})\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
