{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No es necesaria. Analista Junior de Finanzas y Gobierno Corporativo •\\tRecopilación, procesamiento y análisis de información cualitativa y cuantitativa: La recopilación de información se aborda a través de la interacción con clientes, la revisión de documentación interna y la búsqueda de información pública. Posteriormente la misma es procesada en conjunto con los demás consultores con el objetivo de generar conclusiones que se traduzcan en acciones específicas.\\n•\\tAsistencia en el desarrollo de publicaciones: Periódicamente se realizan estudios relacionados a mercados financieros y de capitales que son publicados en diarios y revistas de reconocido prestigio. \\n•\\tAsistencia en el desarrollo de proyectos: En su mayoría, los mismos están dirigidos a identificar ámbitos de mejora en materia de mejores prácticas de Gobierno Corporativo, y eventualmente guiar su implementación. Ello aplica tanto para entidades públicas y privadas.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data from CSV\n",
    "import csv\n",
    "data = []\n",
    "with open('data.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)    \n",
    "    for row in reader:\n",
    "        text = \" \".join(row.values())\n",
    "        data.append(text)\n",
    "        \n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no es necesaria. analista junior de finanzas y gobierno corporativo  \\trecopilación, procesamiento y análisis de información cualitativa y cuantitativa: la recopilación de información se aborda a través de la interacción con clientes, la revisión de documentación interna y la búsqueda de información pública. posteriormente la misma es procesada en conjunto con los demás consultores con el objetivo de generar conclusiones que se traduzcan en acciones específicas.\\n \\tasistencia en el desarrollo de publicaciones: periódicamente se realizan estudios relacionados a mercados financieros y de capitales que son publicados en diarios y revistas de reconocido prestigio. \\n \\tasistencia en el desarrollo de proyectos: en su mayoría, los mismos están dirigidos a identificar ámbitos de mejora en materia de mejores prácticas de gobierno corporativo, y eventualmente guiar su implementación. ello aplica tanto para entidades públicas y privadas.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "punctuations = ['•','/']\n",
    "translator = str.maketrans(\"\".join(punctuations),' '*len(punctuations))\n",
    "\n",
    "proc_data = []\n",
    "for text in data:\n",
    "    text = text.lower()\n",
    "    text = text.translate(translator)\n",
    "    proc_data.append(text)\n",
    "    \n",
    "data = proc_data\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/howl/.virtualenvs/TextClasification/lib/python3.5/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z0\n",
      "Loading default properties from tagger models/spanish.tagger\n",
      "Loading POS tagger from models/spanish.tagger ... done [0.4 sec].\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', './stanford-postagger.jar', 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', 'models/spanish.tagger', '-textFile', '/tmp/tmp7kcpjzju', '-tokenize', 'false', '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf8']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3a919550405f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtag_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspanish_postagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_terms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'kardex'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TextClasification/lib/python3.5/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TextClasification/lib/python3.5/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Run the tagger and get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n\u001b[0;32m--> 107\u001b[0;31m                                        stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/TextClasification/lib/python3.5/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decode_stdoutdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Java command failed : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', './stanford-postagger.jar', 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', 'models/spanish.tagger', '-textFile', '/tmp/tmp7kcpjzju', '-tokenize', 'false', '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf8']"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "spanish_postagger = StanfordPOSTagger('models/spanish.tagger', './stanford-postagger.jar')\n",
    "\n",
    "terms_by_tag = {}\n",
    "terms_by_tag['n'] = set()\n",
    "terms_by_tag['v'] = set()\n",
    "terms_by_tag['a'] = set()\n",
    "terms_by_tag['c'] = set()\n",
    "terms_by_tag['d'] = set()\n",
    "terms_by_tag['f'] = set()\n",
    "terms_by_tag['i'] = set()\n",
    "terms_by_tag['p'] = set()\n",
    "terms_by_tag['r'] = set()\n",
    "terms_by_tag['s'] = set()\n",
    "terms_by_tag['w'] = set()\n",
    "terms_by_tag['z'] = set()\n",
    "\n",
    "\n",
    "for text in data:\n",
    "    tag_terms = spanish_postagger.tag(word_tokenize(text))\n",
    "    for term in tag_terms:\n",
    "        if term[0] == 'kardex':\n",
    "            print(term[1])\n",
    "        for tag in terms_by_tag:\n",
    "            if term[1][0] == tag:\n",
    "                terms_by_tag[tag].add(term[0])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
